#!/usr/bin/env python3
"""
Run the SWE-benchmark tests with actual Claude CLI instead of mocks.
This shows real performance on complex software engineering tasks.
"""

import sys
import tempfile
import shutil
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))
from agent_tree_simple import AgentNode, Context, solve_problem


class BenchmarkRunner:
    """Run benchmarks with real Claude CLI and measure performance."""
    
    def __init__(self):
        self.results = []
        self.start_time = None
        self.workspace = None
    
    def setup_synthetic_web_app(self, base_dir: Path) -> Dict[str, str]:
        """Create the buggy web application files."""
        files = {}
        
        # Controller layer
        controller_content = '''
class UserController:
    def __init__(self, service):
        self.service = service
    
    def get_user_profile(self, user_id, use_new_format=False):
        """Get user profile with optional new format."""
        user_data = self.service.fetch_user(user_id, use_new_format)
        
        if use_new_format:
            # Bug: Not handling the new format properly
            return {
                "id": user_data["id"],
                "name": user_data["name"],
                # Missing: should include full_name when available
            }
        else:
            return user_data
'''
        
        # Service layer
        service_content = '''
class UserService:
    def __init__(self, datastore):
        self.datastore = datastore
    
    def fetch_user(self, user_id, use_new_format=False):
        """Fetch user data from datastore."""
        raw_data = self.datastore.get_user(user_id, use_new_format)
        
        # Bug: Not processing the full_name field
        return {
            "id": raw_data["id"],
            "name": raw_data["name"],
            # Missing: should format full_name when use_new_format is True
        }
'''
        
        # Data layer
        datastore_content = '''
class UserDatastore:
    def __init__(self):
        self.users = {
            1: {"id": 1, "name": "John", "first_name": "John", "last_name": "Doe"},
            2: {"id": 2, "name": "Jane", "first_name": "Jane", "last_name": "Smith"},
        }
    
    def get_user(self, user_id, use_new_format=False):
        """Get user data from storage."""
        user = self.users.get(user_id, {})
        
        if use_new_format:
            # Bug: Constructing full_name but not returning it
            full_name = f"{user.get('first_name', '')} {user.get('last_name', '')}"
            # Should add: user["full_name"] = full_name.strip()
        
        return user
'''
        
        # Test file
        test_content = '''
import unittest
from controller import UserController
from service import UserService
from datastore import UserDatastore

class TestUserProfile(unittest.TestCase):
    def setUp(self):
        self.datastore = UserDatastore()
        self.service = UserService(self.datastore)
        self.controller = UserController(self.service)
    
    def test_old_format(self):
        """Test that old format still works."""
        result = self.controller.get_user_profile(1, use_new_format=False)
        self.assertEqual(result["id"], 1)
        self.assertEqual(result["name"], "John")
        self.assertNotIn("full_name", result)
    
    def test_new_format(self):
        """Test that new format includes full_name."""
        result = self.controller.get_user_profile(1, use_new_format=True)
        self.assertEqual(result["id"], 1)
        self.assertEqual(result["name"], "John")
        # This test will fail due to the bug
        self.assertEqual(result["full_name"], "John Doe")

if __name__ == "__main__":
    unittest.main()
'''
        
        # Write files
        files["controller.py"] = controller_content
        files["service.py"] = service_content
        files["datastore.py"] = datastore_content
        files["test_user.py"] = test_content
        
        for filename, content in files.items():
            filepath = base_dir / filename
            filepath.write_text(content)
        
        return files
    
    def run_benchmark(self, name: str, problem: str, setup_func=None) -> Dict:
        """Run a single benchmark and return results."""
        print(f"\n{'='*60}")
        print(f"Running Benchmark: {name}")
        print(f"{'='*60}")
        
        # Create workspace in tmp directory (in parent)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        tmp_dir = Path(__file__).parent.parent / "tmp"
        tmp_dir.mkdir(exist_ok=True)
        self.workspace = tmp_dir / f"benchmark_{name}_{timestamp}"
        self.workspace.mkdir(exist_ok=True)
        
        # Setup initial files if provided
        if setup_func:
            setup_func(self.workspace)
        
        # Record start time
        self.start_time = time.time()
        
        try:
            # Run the agent tree with real Claude
            print(f"\nProblem: {problem}")
            print("\nRunning agent tree with Claude CLI...")
            print(f"Working directory: {self.workspace}")
            
            # Create a specific workspace for the agent
            agent_workspace = self.workspace / "agent_work"
            agent_workspace.mkdir(exist_ok=True)
            
            # Change to the workspace directory so Claude sees the files
            import os
            original_cwd = os.getcwd()
            os.chdir(self.workspace)
            
            print(f"\nChanged to directory: {os.getcwd()}")
            print("Files in directory:", [f.name for f in Path(".").iterdir()])
            
            # Set logging to INFO to see real-time progress
            import logging
            logging.getLogger().setLevel(logging.INFO)
            
            # Run solve_problem which will use real Claude CLI
            print("\n--- Starting Agent Tree Execution ---")
            result = solve_problem(problem, max_depth=3)
            print("--- Agent Tree Execution Complete ---\n")
            
            # Change back
            os.chdir(original_cwd)
            
            elapsed_time = time.time() - self.start_time
            
            # Analyze results
            benchmark_result = {
                "name": name,
                "success": True,
                "elapsed_time": elapsed_time,
                "problem": problem,
                "solution": result,
                "workspace": str(self.workspace),
                "error": None
            }
            
            # Check if tests would pass (simplified check)
            if "test_user.py" in [f.name for f in self.workspace.iterdir()]:
                print("\nChecking if the bug was fixed...")
                # This is where you would actually run the tests
                # For now, we'll just check if the solution mentions the key fixes
                fixes_mentioned = all(keyword in result.lower() for keyword in ["full_name", "datastore", "service", "controller"])
                benchmark_result["bug_fixed"] = fixes_mentioned
                
            print(f"\nCompleted in {elapsed_time:.2f} seconds")
            
        except Exception as e:
            elapsed_time = time.time() - self.start_time
            benchmark_result = {
                "name": name,
                "success": False,
                "elapsed_time": elapsed_time,
                "problem": problem,
                "solution": None,
                "workspace": str(self.workspace),
                "error": str(e)
            }
            print(f"\nFailed with error: {e}")
        
        self.results.append(benchmark_result)
        return benchmark_result
    
    def generate_report(self) -> str:
        """Generate a summary report of all benchmark results."""
        report = []
        report.append("\n" + "="*60)
        report.append("BENCHMARK RESULTS SUMMARY")
        report.append("="*60)
        
        for result in self.results:
            report.append(f"\nBenchmark: {result['name']}")
            report.append(f"Status: {'âœ… Success' if result['success'] else 'âŒ Failed'}")
            report.append(f"Time: {result['elapsed_time']:.2f} seconds")
            
            if result.get('bug_fixed') is not None:
                report.append(f"Bug Fixed: {'âœ… Yes' if result['bug_fixed'] else 'âŒ No'}")
            
            if result['error']:
                report.append(f"Error: {result['error']}")
            
            report.append(f"Workspace: {result['workspace']}")
            
            if result['solution']:
                report.append("\nSolution Summary:")
                # Show first 500 chars of solution
                solution_preview = result['solution'][:500] + "..." if len(result['solution']) > 500 else result['solution']
                report.append(solution_preview)
        
        report.append("\n" + "="*60)
        return "\n".join(report)


def main():
    """Run all benchmarks with real Claude."""
    runner = BenchmarkRunner()
    
    # Benchmark 1: Synthetic Web App Bug Fix
    runner.run_benchmark(
        name="synthetic_web_app",
        problem="""Fix the use_new_format feature flag implementation in the files in the current directory.

You are in a directory with these Python files that need to be fixed:
- controller.py: UserController class with get_user_profile method
- service.py: UserService class with fetch_user method  
- datastore.py: UserDatastore class with get_user method
- test_user.py: Test file with failing test_new_format test

The bug: When use_new_format=True is passed, the system should return a full_name field but it's missing.

Specific issues to fix:
1. In datastore.py: The get_user method computes full_name but doesn't add it to the returned dict
2. In service.py: The fetch_user method doesn't pass through the full_name field
3. In controller.py: The get_user_profile method doesn't include full_name in its response

Please read each file, identify the bugs, and fix them using the Edit tool. After fixing, verify the changes would make test_new_format pass.""",
        setup_func=lambda workspace: BenchmarkRunner().setup_synthetic_web_app(workspace)
    )
    
    # Benchmark 2: Complex Refactoring (if time permits)
    # runner.run_benchmark(
    #     name="calculator_refactoring",
    #     problem="Refactor the Calculator class to follow SOLID principles...",
    #     setup_func=...
    # )
    
    # Generate and save report
    report = runner.generate_report()
    print(report)
    
    # Save detailed results in results directory
    results_dir = Path(__file__).parent.parent / "results"
    results_dir.mkdir(exist_ok=True)
    results_file = results_dir / f"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(results_file, 'w') as f:
        json.dump(runner.results, f, indent=2)
    
    print(f"\nDetailed results saved to: {results_file}")
    
    # Clean up old runs in tmp directory (keep last 3)
    print("\nðŸ§¹ Cleaning up old temporary runs...")
    tmp_dir = Path(__file__).parent.parent / "tmp"
    if tmp_dir.exists():
        all_runs = sorted(tmp_dir.glob("benchmark_*"))
        if len(all_runs) > 3:
            for old_run in all_runs[:-3]:
                print(f"  Removing: {old_run}")
                shutil.rmtree(old_run)
    print("âœ… Cleanup complete")


if __name__ == "__main__":
    main()